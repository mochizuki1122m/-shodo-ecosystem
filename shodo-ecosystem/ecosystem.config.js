module.exports = {
  apps: [
    {
      name: 'shodo-backend',
      cwd: './backend',
      script: '/usr/bin/python3',
      args: '-m uvicorn src.main:app --host 0.0.0.0 --port 8000',
      instances: 1,
      exec_mode: 'fork',
      env: {
        NODE_ENV: 'production',
        PORT: 8000,
      },
      error_file: './logs/backend-error.log',
      out_file: './logs/backend-out.log',
      log_file: './logs/backend-combined.log',
      time: true,
      watch: false,
      max_memory_restart: '1G',
      min_uptime: '10s',
      max_restarts: 10,
    },
    {
      name: 'shodo-frontend',
      cwd: './frontend',
      script: 'npm',
      args: 'run start',
      env: {
        NODE_ENV: 'production',
        PORT: 3000,
      },
      error_file: './logs/frontend-error.log',
      out_file: './logs/frontend-out.log',
      log_file: './logs/frontend-combined.log',
      time: true,
      watch: false,
      max_memory_restart: '500M',
    },
    {
      name: 'shodo-ai',
      cwd: './ai-server',
      script: 'src/server.js',
      instances: 1,
      env: {
        NODE_ENV: 'production',
        PORT: 8001,
        INFERENCE_ENGINE: process.env.INFERENCE_ENGINE || 'ollama',
        MODEL_NAME: process.env.MODEL_NAME || 'llama2:7b-chat',
      },
      error_file: './logs/ai-error.log',
      out_file: './logs/ai-out.log',
      log_file: './logs/ai-combined.log',
      time: true,
      watch: false,
      max_memory_restart: '2G',
      min_uptime: '30s',
      max_restarts: 5,
    },
  ],
};