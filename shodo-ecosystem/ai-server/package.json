{
  "name": "@shodo/ai-server",
  "version": "1.0.0",
  "private": true,
  "type": "module",
  "scripts": {
    "dev": "node --watch src/server.js",
    "ollama": "INFERENCE_ENGINE=ollama node src/server.js",
    "vllm": "INFERENCE_ENGINE=vllm python src/vllm_server.py",
    "start": "node src/server.js",
    "setup:vllm": "pip install -r requirements.txt && huggingface-cli download TheBloke/Llama-2-7B-Chat-AWQ",
    "test": "vitest",
    "clean": "rm -rf node_modules dist"
  },
  "dependencies": {
    "fastify": "^4.25.2",
    "@fastify/cors": "^8.5.0",
    "@fastify/websocket": "^8.3.1",
    "ollama": "^0.5.0",
    "openai": "^4.24.1",
    "pino": "^8.17.2",
    "pino-pretty": "^10.3.1",
    "dotenv": "^16.3.1"
  },
  "devDependencies": {
    "@types/node": "^20.11.0",
    "vitest": "^1.2.0"
  }
}