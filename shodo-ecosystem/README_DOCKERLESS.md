# ğŸš€ Shodo Ecosystem - Dockerãƒ¬ã‚¹ç’°å¢ƒæ§‹ç¯‰ã‚¬ã‚¤ãƒ‰

**Ollamaï¼ˆæœ€çŸ­èµ·å‹•ï¼‰ â†’ vLLMï¼ˆé«˜æ€§èƒ½ï¼‰ã¸ã®æ®µéšçš„ç§»è¡Œãƒ‘ã‚¹**

## ğŸ“‹ æ¦‚è¦

Dockerã‚’ä½¿ã‚ãšã€ãƒã‚¤ãƒ†ã‚£ãƒ–ç’°å¢ƒã§ç›´æ¥å®Ÿè¡Œã™ã‚‹è»½é‡æ§‹æˆã§ã™ã€‚
- **é–‹ç™º**: `pnpm dev` ã§å…¨ã‚µãƒ¼ãƒ“ã‚¹ä¸€æ‹¬èµ·å‹•
- **æœ¬ç•ª**: PM2/systemdã§å®‰å®šé‹ç”¨
- **å†ç¾æ€§**: asdf + pnpm lockfileã§å®Œå…¨ä¿è¨¼

## ğŸ¯ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

```
é–‹ç™ºç’°å¢ƒ:
  Ollama (7B) + pnpm dev â†’ 5åˆ†ã§èµ·å‹•å¯èƒ½

æœ¬ç•ªç’°å¢ƒï¼ˆæ®µéšçš„ç§»è¡Œï¼‰:
  Step 1: Ollama + PM2
  Step 2: vLLM + PM2
  Step 3: vLLM + systemd
```

## ğŸ› ï¸ ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ

### 1. è‡ªå‹•ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ï¼ˆæ¨å¥¨ï¼‰

```bash
# ãƒªãƒã‚¸ãƒˆãƒªã‚¯ãƒ­ãƒ¼ãƒ³
git clone https://github.com/your-org/shodo-ecosystem.git
cd shodo-ecosystem

# å®Ÿè¡Œæ¨©é™ä»˜ä¸
chmod +x setup.sh

# è‡ªå‹•ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ï¼ˆ10-15åˆ†ï¼‰
./setup.sh

# é–‹ç™ºã‚µãƒ¼ãƒãƒ¼èµ·å‹•
pnpm dev
```

ã“ã‚Œã§ http://localhost:3000 ã«ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½ï¼

### 2. æ‰‹å‹•ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

#### asdfã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```bash
# asdfã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
git clone https://github.com/asdf-vm/asdf.git ~/.asdf --branch v0.13.1
echo '. "$HOME/.asdf/asdf.sh"' >> ~/.bashrc
source ~/.bashrc

# ãƒ—ãƒ©ã‚°ã‚¤ãƒ³è¿½åŠ 
asdf plugin add nodejs
asdf plugin add python
asdf plugin add postgres
asdf plugin add redis

# ãƒ„ãƒ¼ãƒ«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
asdf install
```

#### Ollamaã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```bash
# Linux/Mac
curl -fsSL https://ollama.ai/install.sh | sh

# Windows (WSL2)
# Ollamaã®Windowsç‰ˆã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å¾Œã€WSL2ã‹ã‚‰æ¥ç¶š

# ãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆ4GBç¨‹åº¦ï¼‰
ollama pull llama2:7b-chat
```

#### ä¾å­˜é–¢ä¿‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```bash
# pnpmã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
npm install -g pnpm@8.14.1

# Node.jsä¾å­˜é–¢ä¿‚
pnpm install

# ç’°å¢ƒå¤‰æ•°è¨­å®š
cp .env.example .env
```

#### ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹èµ·å‹•

```bash
# PostgreSQLåˆæœŸåŒ–
initdb -D ~/.asdf/installs/postgres/15.5/data
pg_ctl start -D ~/.asdf/installs/postgres/15.5/data
createdb shodo

# Redisèµ·å‹•
redis-server --daemonize yes
```

## ğŸš€ èµ·å‹•æ–¹æ³•

### é–‹ç™ºç’°å¢ƒï¼ˆæœ€é€Ÿï¼‰

```bash
# å…¨ã‚µãƒ¼ãƒ“ã‚¹ä¸€æ‹¬èµ·å‹•
pnpm dev

# å€‹åˆ¥èµ·å‹•
pnpm dev:backend   # ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰
pnpm dev:frontend  # ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰
pnpm dev:ai       # AIã‚µãƒ¼ãƒãƒ¼ï¼ˆOllamaï¼‰
```

### PM2ã§ã®é‹ç”¨

```bash
# PM2ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pnpm add -g pm2

# ã‚µãƒ¼ãƒ“ã‚¹èµ·å‹•
pnpm start  # pm2 start ecosystem.config.js

# çŠ¶æ…‹ç¢ºèª
pm2 status
pm2 logs

# åœæ­¢
pnpm stop
```

### systemdã§ã®æœ¬ç•ªé‹ç”¨

```bash
# ã‚µãƒ¼ãƒ“ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
sudo cp systemd/*.service /etc/systemd/system/
sudo systemctl daemon-reload

# ã‚µãƒ¼ãƒ“ã‚¹æœ‰åŠ¹åŒ–ãƒ»èµ·å‹•
sudo systemctl enable shodo-backend shodo-frontend shodo-ai
sudo systemctl start shodo-backend shodo-frontend shodo-ai

# çŠ¶æ…‹ç¢ºèª
sudo systemctl status shodo-*
```

## ğŸ”„ Ollama â†’ vLLMç§»è¡Œ

### Step 1: vLLMã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

```bash
# Pythonä»®æƒ³ç’°å¢ƒä½œæˆ
python -m venv venv
source venv/bin/activate

# vLLMã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆCUDAå¿…é ˆï¼‰
pip install vllm

# ãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆAWQé‡å­åŒ–ç‰ˆã€5GBï¼‰
huggingface-cli download TheBloke/Llama-2-7B-Chat-AWQ
```

### Step 2: ç’°å¢ƒå¤‰æ•°åˆ‡ã‚Šæ›¿ãˆ

```bash
# .envã‚’ç·¨é›†
INFERENCE_ENGINE=vllm
VLLM_URL=http://localhost:8000
MODEL_NAME=TheBloke/Llama-2-7B-Chat-AWQ
```

### Step 3: vLLMã‚µãƒ¼ãƒãƒ¼èµ·å‹•

```bash
# é–‹ç™ºç’°å¢ƒ
INFERENCE_ENGINE=vllm pnpm dev

# æœ¬ç•ªç’°å¢ƒï¼ˆsystemdï¼‰
sudo systemctl stop shodo-ai
sudo systemctl edit shodo-ai  # ExecStartã‚’vLLMç”¨ã«å¤‰æ›´
sudo systemctl start shodo-ai
```

## ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ

| æ§‹æˆ | èµ·å‹•æ™‚é–“ | ãƒ¡ãƒ¢ãƒªä½¿ç”¨ | ãƒ¬ã‚¹ãƒãƒ³ã‚¹ | GPUå¿…é ˆ |
|-----|---------|-----------|-----------|---------|
| Ollama (7B) | 30ç§’ | 8GB | 1-2ç§’ | âŒ |
| vLLM (7B AWQ) | 1åˆ† | 6GB | 0.2ç§’ | âœ… |
| vLLM (70B) | 3åˆ† | 40GB | 0.5ç§’ | âœ… |

## ğŸ”§ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«

### .tool-versions (asdf)
```
nodejs 20.11.0
python 3.11.7
postgres 15.5
redis 7.2.4
```

### pnpm-workspace.yaml
```yaml
packages:
  - 'frontend'
  - 'backend'
  - 'ai-server'
```

### ecosystem.config.js (PM2)
```javascript
module.exports = {
  apps: [
    {
      name: 'shodo-backend',
      script: './backend/src/main.js',
      instances: 'max',
      exec_mode: 'cluster'
    },
    // ...
  ]
}
```

## ğŸ› ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### OllamaãŒèµ·å‹•ã—ãªã„

```bash
# ã‚µãƒ¼ãƒ“ã‚¹å†èµ·å‹•
ollama serve

# ãƒãƒ¼ãƒˆç¢ºèª
lsof -i :11434
```

### pnpm devã§ã‚¨ãƒ©ãƒ¼

```bash
# node_moduleså†ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pnpm clean
pnpm install

# ãƒãƒ¼ãƒˆç«¶åˆç¢ºèª
lsof -i :3000
lsof -i :8000
lsof -i :8001
```

### PostgreSQLæ¥ç¶šã‚¨ãƒ©ãƒ¼

```bash
# çŠ¶æ…‹ç¢ºèª
pg_ctl status -D ~/.asdf/installs/postgres/15.5/data

# å†èµ·å‹•
pg_ctl restart -D ~/.asdf/installs/postgres/15.5/data
```

## ğŸ“ˆ ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°æˆ¦ç•¥

```
é–‹ç™ºåˆæœŸ:
  Ollama + é–‹ç™ºã‚µãƒ¼ãƒãƒ¼ï¼ˆpnpm devï¼‰
  â†“
å°è¦æ¨¡æœ¬ç•ª:
  Ollama + PM2ï¼ˆã‚¯ãƒ©ã‚¹ã‚¿ãƒ¢ãƒ¼ãƒ‰ï¼‰
  â†“
ä¸­è¦æ¨¡æœ¬ç•ª:
  vLLM (7B) + PM2 + nginx
  â†“
å¤§è¦æ¨¡æœ¬ç•ª:
  vLLM (70B) + systemd + HAProxy + Redis Cluster
```

## ğŸ¯ æ¨å¥¨æ§‹æˆ

### æœ€å°æ§‹æˆï¼ˆé–‹ç™ºï¼‰
- CPU: 4ã‚³ã‚¢
- RAM: 8GB
- Storage: 20GB
- GPU: ä¸è¦

### æ¨å¥¨æ§‹æˆï¼ˆOllamaæœ¬ç•ªï¼‰
- CPU: 8ã‚³ã‚¢
- RAM: 16GB
- Storage: 50GB
- GPU: ä¸è¦

### é«˜æ€§èƒ½æ§‹æˆï¼ˆvLLMæœ¬ç•ªï¼‰
- CPU: 16ã‚³ã‚¢
- RAM: 32GB
- Storage: 100GB
- GPU: RTX 3090ä»¥ä¸Š

## ğŸš¦ ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯

```bash
# å…¨ã‚µãƒ¼ãƒ“ã‚¹çŠ¶æ…‹ç¢ºèª
curl http://localhost:8000/health  # Backend
curl http://localhost:8001/health  # AI Server
curl http://localhost:3000         # Frontend

# PM2ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°
pm2 monit

# systemdãƒ­ã‚°
journalctl -u shodo-backend -f
```

## ğŸ“ ã¾ã¨ã‚

**æœ€çŸ­ãƒ‘ã‚¹**: 
```bash
./setup.sh && pnpm dev
```

**æœ¬ç•ªç§»è¡Œ**:
```bash
pnpm build && pnpm start
```

**é«˜æ€§èƒ½åŒ–**:
```bash
INFERENCE_ENGINE=vllm pnpm start
```

ã“ã‚Œã§**Dockerä¸è¦**ã§ã€**5åˆ†ã§é–‹ç™ºé–‹å§‹**ã€å¿…è¦ã«å¿œã˜ã¦**æ®µéšçš„ã«æœ¬ç•ªç’°å¢ƒã¸ç§»è¡Œ**ã§ãã¾ã™ï¼