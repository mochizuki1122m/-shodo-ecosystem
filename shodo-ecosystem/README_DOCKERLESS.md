# Shodo Ecosystem - Dockerä¸è¦ç‰ˆã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚¬ã‚¤ãƒ‰

## ğŸš€ ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆï¼ˆWindows/Mac/Linuxï¼‰

### æœ€çŸ­çµŒè·¯ï¼ˆ3åˆ†ã§èµ·å‹•ï¼‰

```bash
# 1. Ollamaã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆå„OSç”¨ï¼‰
# Windows: https://ollama.com/download/windows
# Mac: brew install ollama
# Linux: curl -fsSL https://ollama.ai/install.sh | sh

# 2. ãƒ¢ãƒ‡ãƒ«å–å¾—
ollama pull mistral

# 3. ä¾å­˜ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
npm install
cd frontend && npm install --legacy-peer-deps && cd ..

# 4. èµ·å‹•
npm run dev  # ã¾ãŸã¯ start-simple.bat (Windows)
```

ãƒ–ãƒ©ã‚¦ã‚¶ã§ `http://localhost:3000/simple.html` ã‚’é–‹ã

---

## ğŸ“‹ å‰ææ¡ä»¶

### å¿…é ˆ
- **Node.js 20 LTS** ([ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰](https://nodejs.org/))
- **Python 3.10+** ([ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰](https://www.python.org/))
- **Git** ([ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰](https://git-scm.com/))

### æ¨å¥¨
- **pnpm** - é«˜é€Ÿãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãƒãƒãƒ¼ã‚¸ãƒ£
  ```bash
  npm install -g pnpm
  ```
- **PM2** - ãƒ—ãƒ­ã‚»ã‚¹ç®¡ç†
  ```bash
  npm install -g pm2
  ```

### ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼ˆç”¨é€”åˆ¥ï¼‰
- **Ollama** - ãƒ­ãƒ¼ã‚«ãƒ«LLMï¼ˆæ¨å¥¨ï¼‰
- **CUDA** - GPUåˆ©ç”¨æ™‚ã®ã¿
- **asdf** - ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†

---

## ğŸ› ï¸ ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—æ‰‹é †

### 1. ç’°å¢ƒå¤‰æ•°ã®è¨­å®š

```bash
cp .env.example .env
# .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç·¨é›†
```

ä¸»è¦ãªè¨­å®šé …ç›®ï¼š
```env
# LLMè¨­å®šï¼ˆOllamaä½¿ç”¨æ™‚ï¼‰
LLM_PROVIDER=ollama
OPENAI_BASE_URL=http://localhost:11434/v1
OLLAMA_MODEL=mistral

# ãƒãƒ¼ãƒˆè¨­å®š
API_PORT=8000
FRONTEND_PORT=8080
```

### 2. Ollama ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ï¼ˆæ¨å¥¨ï¼‰

#### Windows
```batch
setup-ollama.bat
```

#### Mac/Linux
```bash
# ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
curl -fsSL https://ollama.ai/install.sh | sh

# èµ·å‹•
ollama serve

# ãƒ¢ãƒ‡ãƒ«å–å¾—
ollama pull mistral
```

### 3. ä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```bash
# pnpmä½¿ç”¨ï¼ˆæ¨å¥¨ï¼‰
pnpm install

# ã¾ãŸã¯ npm
npm install
cd frontend && npm install --legacy-peer-deps && cd ..
```

### 4. ã‚µãƒ¼ãƒ“ã‚¹èµ·å‹•

#### é–‹ç™ºç’°å¢ƒï¼ˆã‚·ãƒ³ãƒ—ãƒ«ï¼‰

**Windows:**
```batch
start-simple.bat
```

**Mac/Linux:**
```bash
# Backend
cd backend && python3 simple_server.py &

# Frontend
cd frontend/public && python3 -m http.server 3000 &
```

#### æœ¬ç•ªç’°å¢ƒï¼ˆPM2ä½¿ç”¨ï¼‰

**Windows:**
```batch
start-production.bat
```

**Mac/Linux:**
```bash
pm2 start ecosystem.config.js
pm2 save
pm2 startup  # è‡ªå‹•èµ·å‹•è¨­å®š
```

---

## ğŸ”§ è©³ç´°è¨­å®š

### LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ã®é¸æŠ

#### Option 1: Ollamaï¼ˆæ¨å¥¨ï¼‰
- **ãƒ¡ãƒªãƒƒãƒˆ**: ç°¡å˜ã€CPUå¯¾å¿œã€ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ä¿è­·
- **ãƒ‡ãƒ¡ãƒªãƒƒãƒˆ**: é€Ÿåº¦ãŒé…ã„å ´åˆãŒã‚ã‚‹

```env
LLM_PROVIDER=ollama
OPENAI_BASE_URL=http://localhost:11434/v1
OLLAMA_MODEL=mistral  # ã¾ãŸã¯ qwen2.5:7b, llama3 ãªã©
```

#### Option 2: vLLMï¼ˆGPUç’°å¢ƒï¼‰
- **ãƒ¡ãƒªãƒƒãƒˆ**: é«˜é€Ÿã€å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«å¯¾å¿œ
- **ãƒ‡ãƒ¡ãƒªãƒƒãƒˆ**: GPUå¿…é ˆã€ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—è¤‡é›‘

```bash
# ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install vllm torch

# èµ·å‹•
python -m vllm.entrypoints.openai.api_server \
  --model mistralai/Mistral-7B-Instruct-v0.2 \
  --port 8001
```

```env
LLM_PROVIDER=vllm
OPENAI_BASE_URL=http://localhost:8001/v1
```

#### Option 3: OpenAI API
- **ãƒ¡ãƒªãƒƒãƒˆ**: æœ€é«˜å“è³ªã€ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ä¸è¦
- **ãƒ‡ãƒ¡ãƒªãƒƒãƒˆ**: ã‚³ã‚¹ãƒˆã€ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆå¿…é ˆ

```env
LLM_PROVIDER=openai
OPENAI_API_KEY=sk-...
```

### ãƒ—ãƒ­ã‚»ã‚¹ç®¡ç†

#### PM2ï¼ˆæ¨å¥¨ï¼‰

```bash
# èµ·å‹•
pm2 start ecosystem.config.js

# ç›£è¦–
pm2 monit

# ãƒ­ã‚°ç¢ºèª
pm2 logs

# å†èµ·å‹•
pm2 restart all

# åœæ­¢
pm2 stop all
```

#### systemdï¼ˆLinuxæœ¬ç•ªç’°å¢ƒï¼‰

```bash
# ã‚µãƒ¼ãƒ“ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
sudo cp systemd/*.service /etc/systemd/system/

# æœ‰åŠ¹åŒ–
sudo systemctl enable --now shodo-backend
sudo systemctl enable --now shodo-frontend
```

### ãƒãƒ¼ã‚¸ãƒ§ãƒ³å›ºå®šï¼ˆãƒãƒ¼ãƒ é–‹ç™ºï¼‰

```bash
# asdfã§ãƒãƒ¼ã‚¸ãƒ§ãƒ³å›ºå®š
echo "nodejs 20.12.2" >> .tool-versions
echo "python 3.11.9" >> .tool-versions
asdf install

# lockãƒ•ã‚¡ã‚¤ãƒ«å³å®ˆ
pnpm install --frozen-lockfile
```

---

## ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°

### ãƒ¡ãƒ¢ãƒªè¨­å®š

```javascript
// ecosystem.config.js
{
  name: 'shodo-frontend',
  max_memory_restart: '500M',  // ãƒ¡ãƒ¢ãƒªåˆ¶é™
  // ...
}
```

### Ollamaæœ€é©åŒ–

```bash
# CPUã‚¹ãƒ¬ãƒƒãƒ‰æ•°ã‚’æŒ‡å®š
OLLAMA_NUM_THREADS=8 ollama serve

# å°å‹ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ï¼ˆé«˜é€ŸåŒ–ï¼‰
ollama pull phi3:mini
```

### ãƒãƒ¼ãƒˆå¤‰æ›´

```env
# .env
API_PORT=3001
FRONTEND_PORT=8081
```

---

## ğŸ” ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### ãƒãƒ¼ãƒˆç«¶åˆ

```bash
# Windows
netstat -ano | findstr :8000
taskkill /PID <PID> /F

# Mac/Linux
lsof -i :8000
kill -9 <PID>
```

### Ollamaæ¥ç¶šã‚¨ãƒ©ãƒ¼

```bash
# ã‚µãƒ¼ãƒ“ã‚¹å†èµ·å‹•
ollama serve

# APIç¢ºèª
curl http://localhost:11434/api/tags
```

### ä¾å­˜é–¢ä¿‚ã‚¨ãƒ©ãƒ¼

```bash
# ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢
npm cache clean --force
rm -rf node_modules package-lock.json
npm install
```

### Pythonç’°å¢ƒã‚¨ãƒ©ãƒ¼

```bash
# ä»®æƒ³ç’°å¢ƒä½œæˆ
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -r requirements.txt
```

---

## ğŸ§¹ ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—

```bash
# PM2ãƒ—ãƒ­ã‚»ã‚¹å‰Šé™¤
pm2 delete all
pm2 save

# ãƒ­ã‚°å‰Šé™¤
rm -rf logs/*

# ä¾å­˜é–¢ä¿‚å‰Šé™¤
rm -rf node_modules
rm -rf frontend/node_modules
rm -rf venv

# Ollamaãƒ¢ãƒ‡ãƒ«å‰Šé™¤
ollama rm mistral
```

---

## ğŸ“ˆ æœ¬ç•ªç’°å¢ƒã¸ã®ç§»è¡Œ

1. **ç’°å¢ƒå¤‰æ•°ã®æœ¬ç•ªè¨­å®š**
   ```env
   NODE_ENV=production
   LOG_LEVEL=warn
   ```

2. **SSL/TLSè¨­å®š**ï¼ˆnginxæ¨å¥¨ï¼‰

3. **ç›£è¦–è¨­å®š**
   - PM2 Plus
   - Datadog
   - New Relic

4. **ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—è¨­å®š**
   - ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å®šæœŸãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
   - ãƒ­ã‚°ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³

5. **ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å¼·åŒ–**
   - ãƒ•ã‚¡ã‚¤ã‚¢ã‚¦ã‚©ãƒ¼ãƒ«è¨­å®š
   - rate limiting
   - CORSè¨­å®š

---

## ğŸ“š å‚è€ƒãƒªãƒ³ã‚¯

- [Ollama Documentation](https://github.com/ollama/ollama)
- [PM2 Documentation](https://pm2.keymetrics.io/)
- [vLLM Documentation](https://docs.vllm.ai/)
- [Node.js Best Practices](https://github.com/goldbergyoni/nodebestpractices)

---

**ã‚µãƒãƒ¼ãƒˆ**: å•é¡ŒãŒç™ºç”Ÿã—ãŸå ´åˆã¯ã€Issueã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚