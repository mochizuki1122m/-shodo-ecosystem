# ğŸ“˜ **shodo ecosystem å®Œå…¨ä»•æ§˜æ›¸å…¼é–‹ç™ºæ›¸ v5.0**
## **GPT-OSSçµ±åˆç‰ˆ - å®Ÿè£…è©³ç´°ä»•æ§˜**

---

## **ç›®æ¬¡**
1. [ã‚·ã‚¹ãƒ†ãƒ æ¦‚è¦](#1-ã‚·ã‚¹ãƒ†ãƒ æ¦‚è¦)
2. [æ©Ÿèƒ½è©³ç´°ä»•æ§˜](#2-æ©Ÿèƒ½è©³ç´°ä»•æ§˜)
3. [æŠ€è¡“ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£](#3-æŠ€è¡“ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£)
4. [å®Ÿè£…ä»•æ§˜](#4-å®Ÿè£…ä»•æ§˜)
5. [ã‚¤ãƒ³ãƒ•ãƒ©æ§‹æˆ](#5-ã‚¤ãƒ³ãƒ•ãƒ©æ§‹æˆ)
6. [é–‹ç™ºæ‰‹é †æ›¸](#6-é–‹ç™ºæ‰‹é †æ›¸)
7. [ãƒ†ã‚¹ãƒˆä»•æ§˜](#7-ãƒ†ã‚¹ãƒˆä»•æ§˜)
8. [é‹ç”¨ä»•æ§˜](#8-é‹ç”¨ä»•æ§˜)

---

## **1. ã‚·ã‚¹ãƒ†ãƒ æ¦‚è¦**

### **1.1 ã‚·ã‚¹ãƒ†ãƒ ã®ç›®çš„ã¨ä¾¡å€¤**

```yaml
ã‚·ã‚¹ãƒ†ãƒ å: shodo ecosystem
ãƒãƒ¼ã‚¸ãƒ§ãƒ³: 5.0
ãƒªãƒªãƒ¼ã‚¹äºˆå®š: 2025å¹´3æœˆ

ç›®çš„:
  ä¸»è¦: éæŠ€è¡“è€…ãŒè‡ªç„¶è¨€èªï¼ˆæ—¥æœ¬èªï¼‰ã§SaaSã‚’å®‰å…¨ã«æ“ä½œã§ãã‚‹çµ±åˆãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ 
  å‰¯æ¬¡: APIã‚³ã‚¹ãƒˆã‚’97.5%å‰Šæ¸›ã—ãªãŒã‚‰15å€ã®é«˜é€ŸåŒ–ã‚’å®Ÿç¾

ä¾¡å€¤ææ¡ˆ:
  ã‚³ã‚¹ãƒˆé¢:
    - æœˆé¡è²»ç”¨: 200ä¸‡å†† â†’ 5ä¸‡å††ï¼ˆ97.5%å‰Šæ¸›ï¼‰
    - åˆæœŸæŠ•è³‡å›å: 1ãƒ¶æœˆ
    - å¹´é–“å‰Šæ¸›é¡: 2,340ä¸‡å††
    
  æ€§èƒ½é¢:
    - ãƒ¬ã‚¹ãƒãƒ³ã‚¹: 3ç§’ â†’ 0.2ç§’ï¼ˆ15å€é«˜é€Ÿï¼‰
    - åŒæ™‚å‡¦ç†: 1 â†’ 50ãƒ¦ãƒ¼ã‚¶ãƒ¼
    - æœˆé–“å‡¦ç†é‡: 10ä¸‡ â†’ 100ä¸‡ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
    
  ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£é¢:
    - ãƒ‡ãƒ¼ã‚¿æ¼æ´©ãƒªã‚¹ã‚¯: ã‚¼ãƒ­ï¼ˆå®Œå…¨ãƒ­ãƒ¼ã‚«ãƒ«å‡¦ç†ï¼‰
    - ã‚³ãƒ³ãƒ—ãƒ©ã‚¤ã‚¢ãƒ³ã‚¹: GDPR/å€‹äººæƒ…å ±ä¿è­·æ³•æº–æ‹ 
    - ç›£æŸ»è¨¼è·¡: å®Œå…¨è¨˜éŒ²

å¯¾è±¡ãƒ¦ãƒ¼ã‚¶ãƒ¼:
  - ECäº‹æ¥­è€…ï¼ˆShopifyé‹å–¶è€…ï¼‰
  - ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°æ‹…å½“è€…
  - ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆ
  - ä¸­å°ä¼æ¥­çµŒå–¶è€…
  - ãƒ•ãƒªãƒ¼ãƒ©ãƒ³ã‚¹
```

### **1.2 ã‚·ã‚¹ãƒ†ãƒ æ§‹æˆè¦ç´ **

| ãƒ¬ã‚¤ãƒ¤ãƒ¼ | ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ | æŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯ | å½¹å‰² |
|---------|--------------|-------------|------|
| **AIå±¤** | GPT-OSSæ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ | GPT-OSS-20B, vLLM | è‡ªç„¶è¨€èªç†è§£ãƒ»ç”Ÿæˆ |
| **UIå±¤** | Webã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ | React 18, TypeScript 5 | ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ |
| **å‡¦ç†å±¤** | NLPã‚µãƒ¼ãƒ“ã‚¹ | äºŒé‡çµŒè·¯è§£æ, Python | æ„å›³è§£æãƒ»æ›–æ˜§æ€§è§£æ±º |
| **èªè¨¼å±¤** | LPRãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ | JWS, TPM 2.0 | ã‚»ã‚­ãƒ¥ã‚¢èªè¨¼ç®¡ç† |
| **çµ±åˆå±¤** | MCPã‚µãƒ¼ãƒãƒ¼ | JSON-RPC 2.0 | ãƒ„ãƒ¼ãƒ«æ¨™æº–åŒ– |
| **å®Ÿè¡Œå±¤** | User-Agent | Playwright, CDP | ãƒ–ãƒ©ã‚¦ã‚¶è‡ªå‹•åŒ– |
| **ãƒ‡ãƒ¼ã‚¿å±¤** | æ°¸ç¶šåŒ– | PostgreSQL, Redis | ãƒ‡ãƒ¼ã‚¿ç®¡ç†ãƒ»ã‚­ãƒ£ãƒƒã‚·ãƒ¥ |

---

## **2. æ©Ÿèƒ½è©³ç´°ä»•æ§˜**

### **2.1 è‡ªç„¶è¨€èªå‡¦ç†æ©Ÿèƒ½**

#### **2.1.1 äºŒé‡çµŒè·¯è§£æã‚¨ãƒ³ã‚¸ãƒ³**

```typescript
interface DualPathAnalysisEngine {
  // æ©Ÿèƒ½ID: F-NLP-001
  // å„ªå…ˆåº¦: Critical
  // ä¾å­˜: GPT-OSS-20B, ãƒ«ãƒ¼ãƒ«ã‚¨ãƒ³ã‚¸ãƒ³
  
  configuration: {
    // ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹è¨­å®š
    ruleEngine: {
      patterns: RegExpPattern[];      // 1000+ãƒ‘ã‚¿ãƒ¼ãƒ³å®šç¾©
      confidence_threshold: 0.8;      // ç¢ºä¿¡åº¦é–¾å€¤
      processing_time_ms: 10;         // ç›®æ¨™å‡¦ç†æ™‚é–“
      cache_ttl_seconds: 3600;        // ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ‰åŠ¹æœŸé™
    };
    
    // AIè§£æè¨­å®š
    aiEngine: {
      model: 'gpt-oss-20b';           // ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«
      temperature: 0.3;                // ç”Ÿæˆã®ç¢ºå®šæ€§
      max_tokens: 2048;                // æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°
      context_window: 128000;          // ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚¦ã‚£ãƒ³ãƒ‰ã‚¦
      quantization: 'INT4';            // é‡å­åŒ–ãƒ¬ãƒ™ãƒ«
    };
    
    // çµ±åˆè¨­å®š
    merger: {
      rule_weight: 0.4;                // ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã®é‡ã¿
      ai_weight: 0.6;                  // AIè§£æã®é‡ã¿
      conflict_resolution: 'ai_first'; // ç«¶åˆè§£æ±ºæ–¹é‡
    };
  };
  
  // ä¸»è¦ãƒ¡ã‚½ãƒƒãƒ‰
  analyze(input: UserInput): Promise<AnalysisResult>;
  refine(result: AnalysisResult, feedback: UserFeedback): Promise<RefinedResult>;
  learn(session: AnalysisSession): Promise<void>;
}
```

##### **å®Ÿè£…è©³ç´°**

```python
# backend/src/services/nlp/dual_path_engine.py
import asyncio
from typing import Dict, List, Optional
from dataclasses import dataclass
import numpy as np

@dataclass
class AnalysisResult:
    intent: str                    # æ“ä½œæ„å›³
    confidence: float              # ç¢ºä¿¡åº¦ (0.0-1.0)
    entities: Dict[str, any]       # æŠ½å‡ºã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£
    service: str                   # å¯¾è±¡ã‚µãƒ¼ãƒ“ã‚¹
    requires_confirmation: bool    # ç¢ºèªè¦å¦
    suggestions: List[str]         # æ”¹å–„ææ¡ˆ
    processing_path: str           # 'rule' | 'ai' | 'merged'

class DualPathEngine:
    """äºŒé‡çµŒè·¯è§£æã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self, config: Dict):
        self.rule_engine = RuleBasedAnalyzer(config['rule_engine'])
        self.ai_engine = GPTOSSAnalyzer(config['ai_engine'])
        self.merger = ResultMerger(config['merger'])
        self.cache = AnalysisCache()
        
    async def analyze(self, input_text: str, context: Optional[Dict] = None) -> AnalysisResult:
        """
        å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã‚’äºŒé‡çµŒè·¯ã§è§£æ
        
        å‡¦ç†ãƒ•ãƒ­ãƒ¼:
        1. ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
        2. å‰å‡¦ç†ï¼ˆæ­£è¦åŒ–ï¼‰
        3. ä¸¦åˆ—è§£æï¼ˆãƒ«ãƒ¼ãƒ« + AIï¼‰
        4. çµæœçµ±åˆ
        5. æ›–æ˜§æ€§è§£æ±º
        6. ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜
        """
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
        cache_key = self._generate_cache_key(input_text, context)
        cached = await self.cache.get(cache_key)
        if cached:
            return cached
            
        # å‰å‡¦ç†
        normalized = self._normalize_input(input_text)
        
        # ä¸¦åˆ—è§£æ
        rule_task = asyncio.create_task(
            self.rule_engine.analyze(normalized, context)
        )
        ai_task = asyncio.create_task(
            self.ai_engine.analyze(normalized, context)
        )
        
        rule_result, ai_result = await asyncio.gather(rule_task, ai_task)
        
        # çµæœçµ±åˆ
        merged_result = self.merger.merge(rule_result, ai_result)
        
        # æ›–æ˜§æ€§è§£æ±º
        if merged_result.confidence < 0.7:
            merged_result = await self._resolve_ambiguity(merged_result, input_text)
            
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜
        await self.cache.set(cache_key, merged_result)
        
        return merged_result
    
    def _normalize_input(self, text: str) -> str:
        """å…¥åŠ›æ­£è¦åŒ–"""
        # å…¨è§’â†’åŠè§’å¤‰æ›
        text = text.translate(str.maketrans(
            'ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™ï¼¡ï¼¢ï¼£ï¼¤ï¼¥ï¼¦ï¼§ï¼¨ï¼©ï¼ªï¼«ï¼¬ï¼­ï¼®ï¼¯ï¼°ï¼±ï¼²ï¼³ï¼´ï¼µï¼¶ï¼·ï¼¸ï¼¹ï¼ºï½ï½‚ï½ƒï½„ï½…ï½†ï½‡ï½ˆï½‰ï½Šï½‹ï½Œï½ï½ï½ï½ï½‘ï½’ï½“ï½”ï½•ï½–ï½—ï½˜ï½™ï½š',
            '0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz'
        ))
        
        # ç©ºç™½æ­£è¦åŒ–
        text = ' '.join(text.split())
        
        return text
    
    async def _resolve_ambiguity(self, result: AnalysisResult, original_input: str) -> AnalysisResult:
        """æ›–æ˜§æ€§è§£æ±º"""
        
        # GPT-OSSã‚’ä½¿ã£ãŸè©³ç´°åŒ–
        clarification_prompt = f"""
        ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›: {original_input}
        
        ç¾åœ¨ã®è§£æçµæœ:
        - æ„å›³: {result.intent}
        - ç¢ºä¿¡åº¦: {result.confidence}
        - ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£: {result.entities}
        
        ã“ã®å…¥åŠ›ã®æ›–æ˜§ãªéƒ¨åˆ†ã‚’ç‰¹å®šã—ã€æ˜ç¢ºåŒ–ã™ã‚‹ãŸã‚ã®è³ªå•ã‚’3ã¤ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
        """
        
        suggestions = await self.ai_engine.generate_clarifications(clarification_prompt)
        result.suggestions = suggestions
        result.requires_confirmation = True
        
        return result
```

#### **2.1.2 æ—¥æœ¬èªç‰¹åŒ–å‡¦ç†**

```python
class JapaneseLanguageProcessor:
    """
    æ©Ÿèƒ½ID: F-NLP-002
    æ—¥æœ¬èªç‰¹æœ‰ã®è¡¨ç¾ã‚’é«˜ç²¾åº¦ã§å‡¦ç†
    """
    
    def __init__(self):
        # æ—¥æœ¬èªç‰¹åŒ–ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ï¼ˆ40-60%åŠ¹ç‡åŒ–ï¼‰
        self.tokenizer = AutoTokenizer.from_pretrained(
            "RakutenAI/rakutenai-7b",
            additional_special_tokens=['<ä¾¡æ ¼>', '<æ—¥ä»˜>', '<å•†å“å>']
        )
        
        # æ›–æ˜§è¡¨ç¾è¾æ›¸
        self.ambiguity_dict = {
            'ã‚‚ã£ã¨': {'type': 'relative', 'default_ratio': 1.2},
            'å°‘ã—': {'type': 'relative', 'default_ratio': 1.1},
            'ã‹ãªã‚Š': {'type': 'relative', 'default_ratio': 1.5},
            'å¤§å¹…ã«': {'type': 'relative', 'default_ratio': 2.0},
            'ç›®ç«‹ã¤': {'type': 'style', 'attributes': ['bold', 'large', 'colorful']},
            'æ§ãˆã‚': {'type': 'style', 'attributes': ['small', 'muted', 'simple']}
        }
        
    def process(self, text: str) -> ProcessedText:
        """
        å‡¦ç†ãƒ•ãƒ­ãƒ¼:
        1. å½¢æ…‹ç´ è§£æ
        2. ä¿‚ã‚Šå—ã‘è§£æ
        3. æ›–æ˜§è¡¨ç¾ã®å…·ä½“åŒ–
        4. ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£æŠ½å‡º
        """
        
        # å½¢æ…‹ç´ è§£æ
        tokens = self.tokenizer.tokenize(text)
        
        # æ›–æ˜§è¡¨ç¾ã®æ¤œå‡ºã¨å…·ä½“åŒ–
        concrete_values = self._concretize_ambiguous_expressions(tokens)
        
        # ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£æŠ½å‡º
        entities = self._extract_entities(tokens)
        
        return ProcessedText(
            original=text,
            tokens=tokens,
            concrete_values=concrete_values,
            entities=entities
        )
    
    def _concretize_ambiguous_expressions(self, tokens: List[str]) -> Dict:
        """
        æ›–æ˜§è¡¨ç¾ã‚’å…·ä½“çš„ãªå€¤ã«å¤‰æ›
        
        ä¾‹:
        - "ä¾¡æ ¼ã‚’ã‚‚ã£ã¨å®‰ã" â†’ {'action': 'decrease', 'target': 'price', 'ratio': 0.8}
        - "å°‘ã—å¤§ãã" â†’ {'action': 'increase', 'target': 'size', 'ratio': 1.1}
        """
        concrete = {}
        
        for i, token in enumerate(tokens):
            if token in self.ambiguity_dict:
                ambiguity = self.ambiguity_dict[token]
                
                # å‰å¾Œã®æ–‡è„ˆã‹ã‚‰å¯¾è±¡ã‚’ç‰¹å®š
                target = self._identify_target(tokens, i)
                
                if ambiguity['type'] == 'relative':
                    concrete[target] = {
                        'action': self._determine_action(tokens, i),
                        'ratio': ambiguity['default_ratio']
                    }
                elif ambiguity['type'] == 'style':
                    concrete[target] = {
                        'attributes': ambiguity['attributes']
                    }
                    
        return concrete
```

### **2.2 ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ»åå¾©ä¿®æ­£æ©Ÿèƒ½**

#### **2.2.1 ã‚µãƒ³ãƒ‰ãƒœãƒƒã‚¯ã‚¹ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚¨ãƒ³ã‚¸ãƒ³**

```typescript
interface PreviewEngine {
  // æ©Ÿèƒ½ID: F-PREVIEW-001
  // å„ªå…ˆåº¦: Critical
  // ç‰¹å¾´: ç„¡é™ã®ä¿®æ­£ã‚µã‚¤ã‚¯ãƒ«ã€æœ¬ç•ªç’°å¢ƒã«å½±éŸ¿ãªã—
  
  capabilities: {
    max_iterations: number;           // ç„¡åˆ¶é™ (å®Ÿè³ª100å›ã¾ã§)
    preview_generation_time_ms: 500;  // ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ç”Ÿæˆæ™‚é–“
    diff_calculation: boolean;        // å·®åˆ†è¨ˆç®—æ©Ÿèƒ½
    visual_comparison: boolean;       // ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«æ¯”è¼ƒ
    version_control: boolean;         // ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†
    rollback: boolean;               // ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿèƒ½
  };
  
  // ã‚³ã‚¢æ©Ÿèƒ½
  generatePreview(changes: Change[]): Promise<Preview>;
  refinePreview(preview: Preview, refinement: string): Promise<Preview>;
  applyToProduction(preview: Preview): Promise<ApplyResult>;
  rollback(version: string): Promise<RollbackResult>;
}
```

##### **å®Ÿè£…è©³ç´°**

```python
# backend/src/services/preview/sandbox_engine.py
import asyncio
import hashlib
from datetime import datetime
from typing import List, Dict, Optional
import json

class SandboxPreviewEngine:
    """
    ã‚µãƒ³ãƒ‰ãƒœãƒƒã‚¯ã‚¹ç’°å¢ƒã§ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’ç”Ÿæˆãƒ»ç®¡ç†
    """
    
    def __init__(self, config: Dict):
        self.sandbox = VirtualEnvironment(config['sandbox'])
        self.version_control = VersionControl()
        self.diff_calculator = DiffCalculator()
        self.visual_renderer = VisualRenderer()
        
    async def generate_preview(self, changes: List[Change], context: Dict) -> Preview:
        """
        ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ç”Ÿæˆ
        
        å‡¦ç†å†…å®¹:
        1. ç¾åœ¨çŠ¶æ…‹ã®ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆ
        2. ä»®æƒ³ç’°å¢ƒã§ã®å¤‰æ›´é©ç”¨
        3. ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°
        4. å·®åˆ†è¨ˆç®—
        5. ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆç”Ÿæˆ
        """
        
        # ç¾åœ¨çŠ¶æ…‹ã‚’ã‚­ãƒ£ãƒ—ãƒãƒ£
        current_state = await self.sandbox.capture_current_state(context['service_id'])
        
        # ä»®æƒ³ç’°å¢ƒã‚’åˆæœŸåŒ–
        virtual_env = await self.sandbox.create_virtual_copy(current_state)
        
        # å¤‰æ›´ã‚’é©ç”¨
        for change in changes:
            await self._apply_change_to_virtual(virtual_env, change)
        
        # ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ç”Ÿæˆ
        visual_preview = await self.visual_renderer.render(virtual_env)
        
        # å·®åˆ†è¨ˆç®—
        diff = self.diff_calculator.calculate(current_state, virtual_env.state)
        
        # ãƒãƒ¼ã‚¸ãƒ§ãƒ³ä¿å­˜
        version_id = self.version_control.save({
            'timestamp': datetime.now(),
            'changes': changes,
            'state': virtual_env.state,
            'parent_version': context.get('parent_version')
        })
        
        return Preview(
            id=self._generate_preview_id(),
            version_id=version_id,
            visual=visual_preview,
            diff=diff,
            changes=changes,
            created_at=datetime.now(),
            revert_token=self._generate_revert_token(version_id),
            confidence_score=self._calculate_confidence(changes)
        )
    
    async def refine_preview(
        self, 
        current_preview: Preview, 
        refinement_request: str
    ) -> Preview:
        """
        ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã®åå¾©ä¿®æ­£
        
        ç‰¹å¾´:
        - å‰å›ã®å¤‰æ›´ã‚’åŸºã«ç´¯ç©çš„ã«ä¿®æ­£
        - è‡ªç„¶è¨€èªã§ã®ä¿®æ­£æŒ‡ç¤ºã«å¯¾å¿œ
        - ä¿®æ­£å±¥æ­´ã‚’å®Œå…¨ä¿æŒ
        """
        
        # GPT-OSSã§ä¿®æ­£æŒ‡ç¤ºã‚’è§£æ
        refinement_analysis = await self.nlp_engine.analyze_refinement(
            refinement_request,
            current_preview.changes
        )
        
        # ä¿®æ­£ã‚’æ—¢å­˜ã®å¤‰æ›´ã«ãƒãƒ¼ã‚¸
        adjusted_changes = self._merge_refinements(
            current_preview.changes,
            refinement_analysis.adjustments
        )
        
        # æ–°ã—ã„ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’ç”Ÿæˆ
        new_preview = await self.generate_preview(
            adjusted_changes,
            {
                'service_id': current_preview.service_id,
                'parent_version': current_preview.version_id
            }
        )
        
        # ä¿®æ­£å±¥æ­´ã‚’è¨˜éŒ²
        new_preview.refinement_history = [
            *current_preview.refinement_history,
            {
                'request': refinement_request,
                'timestamp': datetime.now(),
                'changes_applied': refinement_analysis.adjustments
            }
        ]
        
        return new_preview
    
    def _merge_refinements(
        self, 
        original_changes: List[Change], 
        adjustments: List[Adjustment]
    ) -> List[Change]:
        """
        ä¿®æ­£ã‚’æ—¢å­˜ã®å¤‰æ›´ã«ãƒãƒ¼ã‚¸
        
        ä¾‹:
        å…ƒã®å¤‰æ›´: font-size: 16px â†’ 20px
        ä¿®æ­£æŒ‡ç¤º: "ã‚‚ã†å°‘ã—å°ã•ã"
        çµæœ: font-size: 16px â†’ 18px
        """
        
        merged = original_changes.copy()
        
        for adjustment in adjustments:
            # å¯¾è±¡ã®å¤‰æ›´ã‚’è¦‹ã¤ã‘ã‚‹
            target_index = self._find_target_change(merged, adjustment.target)
            
            if target_index >= 0:
                # æ—¢å­˜ã®å¤‰æ›´ã‚’èª¿æ•´
                if adjustment.type == 'relative':
                    merged[target_index] = self._apply_relative_adjustment(
                        merged[target_index],
                        adjustment
                    )
                elif adjustment.type == 'replace':
                    merged[target_index] = adjustment.new_change
                elif adjustment.type == 'remove':
                    merged.pop(target_index)
            else:
                # æ–°ã—ã„å¤‰æ›´ã¨ã—ã¦è¿½åŠ 
                if adjustment.type == 'add':
                    merged.append(adjustment.new_change)
                    
        return merged
```

#### **2.2.2 ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ **

```python
class PreviewVersionControl:
    """
    æ©Ÿèƒ½ID: F-PREVIEW-002
    ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†
    
    ç‰¹å¾´:
    - ç„¡åˆ¶é™ã®Undo/Redo
    - ãƒ–ãƒ©ãƒ³ãƒãƒ³ã‚°å¯¾å¿œ
    - å·®åˆ†è¡¨ç¤º
    - ã‚¿ã‚¤ãƒ ãƒˆãƒ©ãƒ™ãƒ«æ©Ÿèƒ½
    """
    
    def __init__(self):
        self.versions = []  # å…¨ãƒãƒ¼ã‚¸ãƒ§ãƒ³å±¥æ­´
        self.branches = {}  # ãƒ–ãƒ©ãƒ³ãƒç®¡ç†
        self.current_version = -1
        self.current_branch = 'main'
        
    def save_version(self, preview_data: Dict) -> str:
        """ãƒãƒ¼ã‚¸ãƒ§ãƒ³ä¿å­˜"""
        version = {
            'id': self._generate_version_id(),
            'timestamp': datetime.now(),
            'data': preview_data,
            'parent': self.versions[self.current_version]['id'] if self.current_version >= 0 else None,
            'branch': self.current_branch,
            'metadata': {
                'change_count': len(preview_data.get('changes', [])),
                'confidence': preview_data.get('confidence_score', 0),
                'user_feedback': None
            }
        }
        
        self.versions.append(version)
        self.current_version = len(self.versions) - 1
        
        return version['id']
    
    def undo(self) -> Optional[Dict]:
        """å‰ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã«æˆ»ã‚‹"""
        if self.current_version > 0:
            self.current_version -= 1
            return self.versions[self.current_version]
        return None
    
    def redo(self) -> Optional[Dict]:
        """æ¬¡ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã«é€²ã‚€"""
        if self.current_version < len(self.versions) - 1:
            self.current_version += 1
            return self.versions[self.current_version]
        return None
    
    def create_branch(self, branch_name: str, from_version: Optional[str] = None) -> str:
        """ãƒ–ãƒ©ãƒ³ãƒä½œæˆ"""
        if from_version:
            branch_point = self._find_version(from_version)
        else:
            branch_point = self.current_version
            
        self.branches[branch_name] = {
            'created_at': datetime.now(),
            'branched_from': self.versions[branch_point]['id'],
            'versions': []
        }
        
        return branch_name
    
    def compare_versions(self, version1: str, version2: str) -> Dict:
        """ãƒãƒ¼ã‚¸ãƒ§ãƒ³é–“ã®å·®åˆ†"""
        v1 = self._find_version_data(version1)
        v2 = self._find_version_data(version2)
        
        return {
            'added': self._get_added_changes(v1, v2),
            'removed': self._get_removed_changes(v1, v2),
            'modified': self._get_modified_changes(v1, v2),
            'summary': self._generate_diff_summary(v1, v2)
        }
```

### **2.3 LPR (Login Protocol Receipt) ã‚·ã‚¹ãƒ†ãƒ **

#### **2.3.1 ã‚»ã‚­ãƒ¥ã‚¢èªè¨¼ç®¡ç†**

```typescript
interface LPRSystem {
  // æ©Ÿèƒ½ID: F-AUTH-001
  // ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ¬ãƒ™ãƒ«: Maximum
  // æº–æ‹ : OAuth 2.0, JWT, WebAuthn
  
  security_features: {
    encryption: 'ES256';              // ECDSA with P-256
    key_storage: 'TPM 2.0';          // ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£
    ttl_hours: 2;                    // çŸ­æœŸæœ‰åŠ¹æœŸé™
    device_binding: true;            // ãƒ‡ãƒã‚¤ã‚¹ãƒã‚¤ãƒ³ãƒ‰
    scope_minimization: true;        // æœ€å°æ¨©é™
    audit_logging: true;             // å®Œå…¨ç›£æŸ»
  };
  
  // ã‚³ã‚¢æ©Ÿèƒ½
  generateLPR(session: LoginSession, scopes: string[]): Promise<LPR>;
  validateLPR(lpr: string): Promise<ValidationResult>;
  revokeLPR(lpr: string): Promise<void>;
  auditOperation(lpr: string, operation: Operation): Promise<void>;
}
```

##### **å®Ÿè£…è©³ç´°**

```python
# backend/src/services/auth/lpr_manager.py
import jwt
import hashlib
from cryptography.hazmat.primitives import hashes
from cryptography.hazmat.primitives.asymmetric import ec
from datetime import datetime, timedelta
import json

class LPRManager:
    """
    Login Protocol Receipt ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼
    
    ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£æ©Ÿèƒ½:
    - ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã‚­ãƒ¼ç®¡ç†ï¼ˆTPM 2.0ï¼‰
    - ãƒ‡ãƒã‚¤ã‚¹ãƒã‚¤ãƒ³ãƒ‡ã‚£ãƒ³ã‚°
    - æœ€å°æ¨©é™ã‚¹ã‚³ãƒ¼ãƒ—
    - çŸ­æœŸTTLï¼ˆ2æ™‚é–“ï¼‰
    - å®Œå…¨ç›£æŸ»è¨¼è·¡
    """
    
    def __init__(self, config: Dict):
        self.tpm = TPMInterface(config['tpm'])
        self.key_manager = SecureKeyManager(self.tpm)
        self.audit_logger = AuditLogger(config['audit'])
        self.scope_validator = ScopeValidator()
        
    async def generate_lpr(
        self,
        login_session: LoginSession,
        requested_scopes: List[str]
    ) -> LPR:
        """
        LPRç”Ÿæˆ
        
        ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è¦ä»¶:
        1. ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ˜ç¤ºçš„åŒæ„
        2. æœ€å°æ¨©é™ã®åŸå‰‡
        3. ãƒ‡ãƒã‚¤ã‚¹ãƒã‚¤ãƒ³ãƒ‡ã‚£ãƒ³ã‚°
        4. çŸ­æœŸæœ‰åŠ¹æœŸé™
        """
        
        # ãƒ­ã‚°ã‚¤ãƒ³ã‚»ãƒƒã‚·ãƒ§ãƒ³æ¤œè¨¼
        if not await self._verify_login_session(login_session):
            raise SecurityException("Invalid login session")
        
        # ã‚¹ã‚³ãƒ¼ãƒ—æœ€å°åŒ–
        minimal_scopes = self.scope_validator.minimize_scopes(
            requested_scopes,
            login_session.user_permissions
        )
        
        # ãƒ‡ãƒã‚¤ã‚¹ãƒ•ã‚£ãƒ³ã‚¬ãƒ¼ãƒ—ãƒªãƒ³ãƒˆç”Ÿæˆ
        device_fingerprint = await self._generate_device_fingerprint()
        
        # LPRãƒšã‚¤ãƒ­ãƒ¼ãƒ‰æ§‹ç¯‰
        lpr_payload = {
            'lpr_version': '1.0',
            'subject_pseudonym': self._pseudonymize_user(login_session.user_id),
            'issued_at': datetime.utcnow().isoformat(),
            'expires_at': (datetime.utcnow() + timedelta(hours=2)).isoformat(),
            'origin_allowlist': login_session.allowed_origins,
            'scope_allowlist': minimal_scopes,
            'session_fingerprint': {
                'alg': 'SHA-256',
                'value': self._hash_session(login_session)
            },
            'device_fingerprint': device_fingerprint,
            'ua_public_key': await self.key_manager.get_public_key(),
            'policy': {
                'rate_limit_rps': 0.2,  # 5ç§’ã«1ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
                'dom_action_limit': 200,
                'data_minimization': True,
                'require_mfa': login_session.mfa_verified
            },
            'audit_endpoint': 'mcp://audit/submit',
            'revocation_endpoint': 'mcp://login/revoke',
            'nonce': self._generate_nonce()
        }
        
        # TPMã§ç½²å
        signature = await self.tpm.sign(
            json.dumps(lpr_payload, sort_keys=True).encode(),
            self.key_manager.get_signing_key_handle()
        )
        
        # JWSå½¢å¼ã§è¿”å´
        lpr_jws = self._create_jws(lpr_payload, signature)
        
        # ç›£æŸ»ãƒ­ã‚°
        await self.audit_logger.log({
            'event': 'lpr_generated',
            'user': lpr_payload['subject_pseudonym'],
            'scopes': minimal_scopes,
            'expires_at': lpr_payload['expires_at'],
            'device': device_fingerprint
        })
        
        return LPR(
            jws=lpr_jws,
            payload=lpr_payload,
            created_at=datetime.utcnow()
        )
    
    async def validate_scope(
        self,
        lpr: LPR,
        operation: Operation
    ) -> ValidationResult:
        """
        ã‚¹ã‚³ãƒ¼ãƒ—æ¤œè¨¼
        
        æ¤œè¨¼é …ç›®:
        1. URLãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°
        2. HTTPãƒ¡ã‚½ãƒƒãƒ‰ç¢ºèª
        3. ãƒ¬ãƒ¼ãƒˆåˆ¶é™ãƒã‚§ãƒƒã‚¯
        4. ãƒ‡ãƒ¼ã‚¿ã‚¢ã‚¯ã‚»ã‚¹ç¯„å›²
        """
        
        # æœ‰åŠ¹æœŸé™ãƒã‚§ãƒƒã‚¯
        if datetime.utcnow() > datetime.fromisoformat(lpr.payload['expires_at']):
            return ValidationResult(
                valid=False,
                reason='LPR expired',
                action='regenerate'
            )
        
        # ãƒ‡ãƒã‚¤ã‚¹ãƒã‚¤ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ç¢ºèª
        current_device = await self._generate_device_fingerprint()
        if current_device != lpr.payload['device_fingerprint']:
            return ValidationResult(
                valid=False,
                reason='Device mismatch',
                action='block'
            )
        
        # ã‚¹ã‚³ãƒ¼ãƒ—ãƒãƒƒãƒãƒ³ã‚°
        scope_match = False
        for scope in lpr.payload['scope_allowlist']:
            pattern = self._parse_scope_pattern(scope)
            if self._matches_pattern(operation, pattern):
                scope_match = True
                break
                
        if not scope_match:
            await self.audit_logger.log({
                'event': 'scope_violation',
                'lpr_id': lpr.payload['subject_pseudonym'],
                'attempted_operation': operation.to_dict(),
                'timestamp': datetime.utcnow()
            })
            
            return ValidationResult(
                valid=False,
                reason=f'Operation {operation} not in scope',
                action='deny'
            )
        
        # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ãƒã‚§ãƒƒã‚¯
        if not await self._check_rate_limit(lpr, operation):
            return ValidationResult(
                valid=False,
                reason='Rate limit exceeded',
                action='throttle'
            )
        
        return ValidationResult(
            valid=True,
            reason='All checks passed'
        )
```

### **2.4 ãƒ–ãƒ©ã‚¦ã‚¶çµ±åˆãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰**

#### **2.4.1 SaaSè‡ªå‹•æ¤œå‡ºã‚·ã‚¹ãƒ†ãƒ **

```typescript
interface SaaSDetectionSystem {
  // æ©Ÿèƒ½ID: F-DASH-001
  // æ¤œå‡ºå¯èƒ½ã‚µãƒ¼ãƒ“ã‚¹: 100+
  // ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ›´æ–°: 5ç§’é–“éš”
  
  detection_methods: {
    cookie_analysis: boolean;         // Cookieè§£æ
    url_pattern_matching: boolean;    // URLãƒ‘ã‚¿ãƒ¼ãƒ³
    dom_inspection: boolean;          // DOMæ§‹é€ è§£æ
    api_endpoint_detection: boolean;  // APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆæ¤œå‡º
    localStorage_scan: boolean;       // LocalStorageèµ°æŸ»
  };
  
  // æ¤œå‡ºãƒ‘ã‚¿ãƒ¼ãƒ³DB
  patterns: Map<string, ServicePattern>;
  
  // ãƒ¡ã‚½ãƒƒãƒ‰
  scanBrowser(): Promise<DetectedService[]>;
  monitorChanges(): Observable<ServiceChange>;
  getServiceCapabilities(service: string): Promise<Capability[]>;
}
```

##### **å®Ÿè£…è©³ç´°**

```python
# backend/src/services/dashboard/saas_detector.py
import asyncio
from typing import List, Dict, Optional
import re
from datetime import datetime

class SaaSDetector:
    """
    SaaSè‡ªå‹•æ¤œå‡ºã‚·ã‚¹ãƒ†ãƒ 
    100ä»¥ä¸Šã®ã‚µãƒ¼ãƒ“ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ä¿æŒ
    """
    
    def __init__(self):
        # ã‚µãƒ¼ãƒ“ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³å®šç¾©ï¼ˆ100+ã‚µãƒ¼ãƒ“ã‚¹ï¼‰
        self.patterns = {
            'shopify': {
                'domains': [
                    r'\.myshopify\.com',
                    r'admin\.shopify\.com',
                    r'partners\.shopify\.com'
                ],
                'cookies': ['_shopify_s', '_shopify_y', '_s', '_shopify_fs'],
                'localStorage_keys': ['shopify_user', 'shopify_theme'],
                'api_endpoints': ['/admin/api/', '/graphql'],
                'dom_selectors': ['[data-shopify-editor]', '.shopify-section'],
                'capabilities': ['products', 'orders', 'customers', 'analytics'],
                'display_name': 'Shopify',
                'icon': 'ğŸ›ï¸',
                'categories': ['EC', 'Commerce'],
                'priority': 1
            },
            'gmail': {
                'domains': [r'mail\.google\.com'],
                'cookies': ['GMAIL_AT', 'COMPASS', 'GMAIL_STAT'],
                'api_endpoints': ['/mail/u/'],
                'dom_selectors': ['[role="main"]', '[gh="tl"]'],
                'capabilities': ['email', 'labels', 'filters'],
                'display_name': 'Gmail',
                'icon': 'ğŸ“§',
                'categories': ['Email', 'Communication']
            },
            'stripe': {
                'domains': [r'dashboard\.stripe\.com'],
                'cookies': ['__stripe_sid', '__stripe_mid'],
                'api_endpoints': ['/v1/', '/graphql'],
                'capabilities': ['payments', 'subscriptions', 'invoices'],
                'display_name': 'Stripe',
                'icon': 'ğŸ’³',
                'categories': ['Payment', 'Finance']
            },
            # ... 100+ ã‚µãƒ¼ãƒ“ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³
        }
        
        self.browser_controller = BrowserController()
        self.cache = DetectionCache(ttl=300)  # 5åˆ†ã‚­ãƒ£ãƒƒã‚·ãƒ¥
        
    async def scan_browser(self) -> List[DetectedService]:
        """
        ãƒ–ãƒ©ã‚¦ã‚¶å…¨ä½“ã‚’ã‚¹ã‚­ãƒ£ãƒ³ã—ã¦SaaSã‚’æ¤œå‡º
        
        å‡¦ç†:
        1. å…¨ã‚¿ãƒ–ã‚’å–å¾—
        2. å„ã‚¿ãƒ–ã§ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°
        3. ãƒ­ã‚°ã‚¤ãƒ³çŠ¶æ…‹ç¢ºèª
        4. æ©Ÿèƒ½æ¤œå‡º
        5. çµæœçµ±åˆ
        """
        
        detected_services = []
        
        # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªãƒ–ãƒ©ã‚¦ã‚¶ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
        contexts = await self.browser_controller.get_contexts()
        
        for context in contexts:
            pages = await context.pages()
            
            for page in pages:
                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
                page_url = page.url()
                cached = self.cache.get(page_url)
                if cached:
                    detected_services.append(cached)
                    continue
                
                # ã‚µãƒ¼ãƒ“ã‚¹æ¤œå‡º
                service = await self._detect_service(page)
                if service:
                    # ãƒ­ã‚°ã‚¤ãƒ³çŠ¶æ…‹ç¢ºèª
                    service.is_logged_in = await self._check_login_status(page, service)
                    
                    # åˆ©ç”¨å¯èƒ½æ©Ÿèƒ½ã®æ¤œå‡º
                    if service.is_logged_in:
                        service.available_capabilities = await self._detect_capabilities(page, service)
                    
                    detected_services.append(service)
                    self.cache.set(page_url, service)
                    
        return self._deduplicate_services(detected_services)
    
    async def _detect_service(self, page) -> Optional[DetectedService]:
        """
        ãƒšãƒ¼ã‚¸ã‹ã‚‰ã‚µãƒ¼ãƒ“ã‚¹ã‚’æ¤œå‡º
        
        æ¤œå‡ºæ–¹æ³•ã®å„ªå…ˆé †ä½:
        1. URL ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°ï¼ˆæœ€é€Ÿï¼‰
        2. Cookie åˆ†æ
        3. LocalStorage æ¤œæŸ»
        4. DOM æ§‹é€ è§£æ
        5. API ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆæ¤œå‡º
        """
        
        url = page.url()
        
        for service_key, pattern in self.patterns.items():
            # URL ãƒãƒƒãƒãƒ³ã‚°
            for domain_pattern in pattern.get('domains', []):
                if re.search(domain_pattern, url):
                    # è¿½åŠ æ¤œè¨¼
                    confidence = 0.5  # URLãƒãƒƒãƒã§50%
                    
                    # Cookie æ¤œè¨¼
                    cookies = await page.context().cookies(url)
                    cookie_names = {c['name'] for c in cookies}
                    matching_cookies = len(
                        set(pattern.get('cookies', [])) & cookie_names
                    )
                    if matching_cookies > 0:
                        confidence += 0.2 * (matching_cookies / len(pattern.get('cookies', [])))
                    
                    # DOM æ¤œè¨¼
                    for selector in pattern.get('dom_selectors', [])[:2]:  # æœ€åˆã®2ã¤ã ã‘ãƒã‚§ãƒƒã‚¯
                        try:
                            element = await page.query_selector(selector, timeout=1000)
                            if element:
                                confidence += 0.15
                        except:
                            pass
                            
                    if confidence >= 0.7:  # 70%ä»¥ä¸Šã®ç¢ºä¿¡åº¦ã§æ¤œå‡º
                        return DetectedService(
                            service_key=service_key,
                            display_name=pattern['display_name'],
                            icon=pattern['icon'],
                            url=url,
                            confidence=confidence,
                            categories=pattern['categories'],
                            detected_at=datetime.utcnow(),
                            page_reference=page
                        )
                        
        return None
    
    async def _check_login_status(
        self,
        page,
        service: DetectedService
    ) -> bool:
        """
        ãƒ­ã‚°ã‚¤ãƒ³çŠ¶æ…‹ã®ç¢ºèª
        
        ç¢ºèªæ–¹æ³•:
        1. ã‚»ãƒƒã‚·ãƒ§ãƒ³Cookieã®å­˜åœ¨
        2. ãƒ­ã‚°ã‚¢ã‚¦ãƒˆãƒœã‚¿ãƒ³ã®å­˜åœ¨
        3. ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±ã®è¡¨ç¤º
        4. APIãƒ¬ã‚¹ãƒãƒ³ã‚¹ç¢ºèª
        """
        
        pattern = self.patterns[service.service_key]
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³Cookieç¢ºèª
        cookies = await page.context().cookies(service.url)
        session_cookies = [
            c for c in cookies
            if c['name'] in pattern.get('cookies', [])
            and c.get('expires', 0) > datetime.utcnow().timestamp()
        ]
        
        if session_cookies:
            # è¿½åŠ ã®DOMç¢ºèª
            try:
                # ãƒ­ã‚°ã‚¢ã‚¦ãƒˆãƒœã‚¿ãƒ³ã‚„ãƒ¦ãƒ¼ã‚¶ãƒ¼åã®å­˜åœ¨ç¢ºèª
                user_indicators = [
                    '[data-user]',
                    '[aria-label*="account"]',
                    'button:has-text("ãƒ­ã‚°ã‚¢ã‚¦ãƒˆ")',
                    'button:has-text("Sign out")',
                    'button:has-text("Log out")'
                ]
                
                for indicator in user_indicators:
                    element = await page.query_selector(indicator, timeout=1000)
                    if element:
                        return True
                        
            except:
                pass
                
        return False
```

### **2.5 Tool-Agnostic MCPçµ±åˆ**

#### **2.5.1 å‹•çš„ãƒ„ãƒ¼ãƒ«çµ±åˆã‚·ã‚¹ãƒ†ãƒ **

```typescript
interface MCPIntegration {
  // æ©Ÿèƒ½ID: F-MCP-001
  // ç‰¹å¾´: UIæ”¹ä¿®ä¸è¦ã§æ–°ãƒ„ãƒ¼ãƒ«è¿½åŠ å¯èƒ½
  
  capabilities: {
    auto_discovery: boolean;          // ãƒ„ãƒ¼ãƒ«è‡ªå‹•ç™ºè¦‹
    dynamic_form_generation: boolean; // å‹•çš„ãƒ•ã‚©ãƒ¼ãƒ ç”Ÿæˆ
    schema_validation: boolean;       // ã‚¹ã‚­ãƒ¼ãƒæ¤œè¨¼
    hot_reload: boolean;             // ãƒ›ãƒƒãƒˆãƒªãƒ­ãƒ¼ãƒ‰å¯¾å¿œ
    versioning: boolean;             // ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†
  };
  
  // ãƒ„ãƒ¼ãƒ«ç®¡ç†
  registerTool(manifest: ToolManifest): Promise<void>;
  unregisterTool(toolId: string): Promise<void>;
  invokeTool(toolId: string, params: any): Promise<any>;
}
```

##### **å®Ÿè£…è©³ç´°**

```python
# backend/src/services/mcp/mcp_server.py
import json
from typing import Dict, Any, List
import jsonschema
from jsonschema import validate

class MCPServer:
    """
    Model Context Protocol ã‚µãƒ¼ãƒãƒ¼
    
    ç‰¹å¾´:
    - ãƒ„ãƒ¼ãƒ«ã®å‹•çš„ç™»éŒ²ãƒ»å‰Šé™¤
    - JSON Schemaãƒ™ãƒ¼ã‚¹ã®æ¤œè¨¼
    - çµ±ä¸€ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
    - ãƒ›ãƒƒãƒˆãƒªãƒ­ãƒ¼ãƒ‰å¯¾å¿œ
    """
    
    def __init__(self, config: Dict):
        self.tools = {}
        self.schema_validator = jsonschema.Draft7Validator
        self.execution_engine = ExecutionEngine(config['execution'])
        self.audit_logger = AuditLogger(config['audit'])
        
    async def register_tool(self, manifest: Dict) -> str:
        """
        æ–°ãƒ„ãƒ¼ãƒ«ã®ç™»éŒ²ï¼ˆUIæ”¹ä¿®ä¸è¦ï¼‰
        
        å‡¦ç†:
        1. ãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆæ¤œè¨¼
        2. ã‚¹ã‚­ãƒ¼ãƒæ¤œè¨¼
        3. ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆç¢ºèª
        4. ãƒ„ãƒ¼ãƒ«ç™»éŒ²
        5. UIé€šçŸ¥
        """
        
        # ãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆæ¤œè¨¼
        self._validate_manifest(manifest)
        
        tool_id = manifest['id']
        
        # æ—¢å­˜ãƒ„ãƒ¼ãƒ«ã®ä¸Šæ›¸ãé˜²æ­¢
        if tool_id in self.tools and not manifest.get('force', False):
            raise ValueError(f"Tool {tool_id} already exists")
        
        # ãƒ„ãƒ¼ãƒ«ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆä½œæˆ
        tool = MCPTool(
            id=tool_id,
            manifest=manifest,
            input_schema=manifest['input_schema'],
            output_schema=manifest['output_schema'],
            endpoint=manifest['endpoint'],
            scopes=manifest.get('scopes', []),
            policy=manifest.get('policy', {})
        )
        
        # ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆåˆ°é”ç¢ºèª
        if not await self._verify_endpoint(tool.endpoint):
            raise ConnectionError(f"Cannot reach endpoint: {tool.endpoint}")
        
        # ç™»éŒ²
        self.tools[tool_id] = tool
        
        # UIã«é€šçŸ¥ï¼ˆè‡ªå‹•çš„ã«ãƒ•ã‚©ãƒ¼ãƒ ãŒç”Ÿæˆã•ã‚Œã‚‹ï¼‰
        await self._notify_ui('tool_registered', {
            'tool_id': tool_id,
            'manifest': manifest
        })
        
        return tool_id
    
    async def invoke_tool(
        self,
        tool_id: str,
        params: Dict[str, Any],
        context: ExecutionContext
    ) -> Any:
        """
        ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œ
        
        ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£:
        1. å…¥åŠ›æ¤œè¨¼
        2. æ¨©é™ç¢ºèª
        3. ãƒ¬ãƒ¼ãƒˆåˆ¶é™
        4. å®Ÿè¡Œ
        5. å‡ºåŠ›æ¤œè¨¼
        6. ç›£æŸ»è¨˜éŒ²
        """
        
        tool = self.tools.get(tool_id)
        if not tool:
            raise ValueError(f"Tool not found: {tool_id}")
        
        # å…¥åŠ›æ¤œè¨¼
        try:
            validate(instance=params, schema=tool.input_schema)
        except jsonschema.exceptions.ValidationError as e:
            raise ValueError(f"Invalid input: {e.message}")
        
        # æ¨©é™ç¢ºèª
        if not await self._check_permissions(tool, context):
            raise PermissionError(f"Insufficient permissions for {tool_id}")
        
        # ãƒ¬ãƒ¼ãƒˆåˆ¶é™
        if not await self._check_rate_limit(tool, context):
            raise RateLimitError(f"Rate limit exceeded for {tool_id}")
        
        # å®Ÿè¡Œ
        result = await self.execution_engine.execute(
            tool=tool,
            params=params,
            context=context
        )
        
        # å‡ºåŠ›æ¤œè¨¼
        try:
            validate(instance=result, schema=tool.output_schema)
        except jsonschema.exceptions.ValidationError as e:
            raise ValueError(f"Invalid output from tool: {e.message}")
        
        # ç›£æŸ»è¨˜éŒ²
        await self.audit_logger.log({
            'tool_id': tool_id,
            'params': self._sanitize_params(params),
            'result_summary': self._summarize_result(result),
            'context': context.to_dict(),
            'timestamp': datetime.utcnow()
        })
        
        return result
    
    def _validate_manifest(self, manifest: Dict):
        """ãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆæ¤œè¨¼"""
        required_fields = [
            'id', 'version', 'input_schema', 
            'output_schema', 'endpoint'
        ]
        
        for field in required_fields:
            if field not in manifest:
                raise ValueError(f"Missing required field: {field}")
        
        # JSON Schemaæ¤œè¨¼
        for schema_field in ['input_schema', 'output_schema']:
            try:
                self.schema_validator.check_schema(manifest[schema_field])
            except jsonschema.exceptions.SchemaError as e:
                raise ValueError(f"Invalid {schema_field}: {e.message}")
```

---

## **3. æŠ€è¡“ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**

### **3.1 GPT-OSSçµ±åˆã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**

```yaml
AIå‡¦ç†å±¤:
  æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³:
    primary:
      model: GPT-OSS-20B
      quantization: INT4 (AWQ)
      vram_usage: 16GB
      throughput: 54 tokens/sec
      context_window: 128K
      
    secondary:
      model: Swallow-70B
      purpose: æ—¥æœ¬èªç‰¹åŒ–å‡¦ç†
      quantization: Q4_K_M
      vram_usage: 35GB
      
    lightweight:
      model: Phi-4
      purpose: ç°¡æ˜“ã‚¿ã‚¹ã‚¯
      vram_usage: 8GB
      throughput: 100 tokens/sec
      
  æ¨è«–ã‚µãƒ¼ãƒãƒ¼:
    framework: vLLM
    features:
      - PagedAttentionï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡24å€ï¼‰
      - Continuous Batchingï¼ˆã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ5å€ï¼‰
      - Prefix Cachingï¼ˆå…±é€šãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé«˜é€ŸåŒ–ï¼‰
      - Speculative Decodingï¼ˆæ¨è«–é«˜é€ŸåŒ–ï¼‰
      
  æœ€é©åŒ–:
    - Flash Attention 2
    - KV Cacheåœ§ç¸®
    - Dynamic Batching
    - Model Parallelismï¼ˆå¿…è¦æ™‚ï¼‰
```

### **3.2 ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“æ§‹æˆ**

```mermaid
graph TB
    subgraph "Client Layer"
        WEB[React SPA]
        MOBILE[PWA]
    end
    
    subgraph "API Gateway"
        GW[Kong / Nginx]
        AUTH[Auth Service]
    end
    
    subgraph "Application Layer"
        API[FastAPI]
        GQL[GraphQL]
        WS[WebSocket]
    end
    
    subgraph "AI Processing"
        VLLM[vLLM Server]
        GPT[GPT-OSS-20B]
        CACHE[Embedding Cache]
    end
    
    subgraph "Core Services"
        NLP[NLP Engine]
        PREVIEW[Preview Engine]
        LPR[LPR Manager]
        MCP[MCP Server]
    end
    
    subgraph "Execution"
        UA[User-Agent Farm]
        CDP[CDP Controllers]
    end
    
    subgraph "Data"
        PG[(PostgreSQL)]
        REDIS[(Redis)]
        S3[Object Storage]
    end
    
    WEB --> GW
    GW --> API
    API --> NLP
    NLP --> VLLM
    VLLM --> GPT
    API --> LPR
    API --> MCP
    MCP --> UA
```

---

## **4. å®Ÿè£…ä»•æ§˜**

### **4.1 ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹é€ **

```
shodo-ecosystem/
â”œâ”€â”€ docker-compose.yml           # é–‹ç™ºç’°å¢ƒå®šç¾©
â”œâ”€â”€ Makefile                     # ãƒ“ãƒ«ãƒ‰ãƒ»ãƒ‡ãƒ—ãƒ­ã‚¤è‡ªå‹•åŒ–
â”‚
â”œâ”€â”€ frontend/                    # Reactã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ features/           # æ©Ÿèƒ½åˆ¥ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
â”‚   â”‚   â”‚   â”œâ”€â”€ nlp/           # è‡ªç„¶è¨€èªå…¥åŠ›
â”‚   â”‚   â”‚   â”œâ”€â”€ preview/       # ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼æ©Ÿèƒ½
â”‚   â”‚   â”‚   â”œâ”€â”€ dashboard/     # ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰
â”‚   â”‚   â”‚   â””â”€â”€ auth/          # èªè¨¼
â”‚   â”‚   â”œâ”€â”€ components/         # å…±é€šã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
â”‚   â”‚   â”œâ”€â”€ hooks/             # ã‚«ã‚¹ã‚¿ãƒ ãƒ•ãƒƒã‚¯
â”‚   â”‚   â”œâ”€â”€ services/          # APIé€šä¿¡
â”‚   â”‚   â””â”€â”€ store/             # Redux Store
â”‚   â””â”€â”€ package.json
â”‚
â”œâ”€â”€ backend/                    # FastAPIã‚µãƒ¼ãƒãƒ¼
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ api/               # APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
â”‚   â”‚   â”‚   â”œâ”€â”€ v1/
â”‚   â”‚   â”‚   â””â”€â”€ graphql/
â”‚   â”‚   â”œâ”€â”€ services/          # ãƒ“ã‚¸ãƒã‚¹ãƒ­ã‚¸ãƒƒã‚¯
â”‚   â”‚   â”‚   â”œâ”€â”€ nlp/          # NLPå‡¦ç†
â”‚   â”‚   â”‚   â”œâ”€â”€ preview/      # ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ç”Ÿæˆ
â”‚   â”‚   â”‚   â”œâ”€â”€ lpr/          # LPRç®¡ç†
â”‚   â”‚   â”‚   â”œâ”€â”€ mcp/          # MCPã‚µãƒ¼ãƒãƒ¼
â”‚   â”‚   â”‚   â””â”€â”€ dashboard/    # ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰
â”‚   â”‚   â”œâ”€â”€ models/            # ãƒ‡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«
â”‚   â”‚   â”œâ”€â”€ workers/           # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã‚¸ãƒ§ãƒ–
â”‚   â”‚   â””â”€â”€ utils/             # ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ Dockerfile
â”‚
â”œâ”€â”€ ai-server/                  # GPT-OSSæ¨è«–ã‚µãƒ¼ãƒãƒ¼
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ vllm_server.py    # vLLMã‚µãƒ¼ãƒãƒ¼
â”‚   â”‚   â”œâ”€â”€ model_loader.py   # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ€ãƒ¼
â”‚   â”‚   â””â”€â”€ optimizations/    # æœ€é©åŒ–ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
â”‚   â”œâ”€â”€ models/                # ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
â”‚   â””â”€â”€ Dockerfile
â”‚
â”œâ”€â”€ user-agent/                 # Playwrightå®Ÿè£…
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ browser/          # ãƒ–ãƒ©ã‚¦ã‚¶åˆ¶å¾¡
â”‚   â”‚   â”œâ”€â”€ detectors/        # SaaSæ¤œå‡º
â”‚   â”‚   â”œâ”€â”€ executors/        # æ“ä½œå®Ÿè¡Œ
â”‚   â”‚   â””â”€â”€ security/         # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£
â”‚   â””â”€â”€ Dockerfile
â”‚
â”œâ”€â”€ infrastructure/            # ã‚¤ãƒ³ãƒ•ãƒ©å®šç¾©
â”‚   â”œâ”€â”€ terraform/            # IaC
â”‚   â”œâ”€â”€ kubernetes/           # K8s manifests
â”‚   â””â”€â”€ monitoring/           # Prometheus/Grafana
â”‚
â””â”€â”€ tests/                     # ãƒ†ã‚¹ãƒˆ
    â”œâ”€â”€ unit/
    â”œâ”€â”€ integration/
    â””â”€â”€ e2e/
```

### **4.2 ç’°å¢ƒå¤‰æ•°è¨­å®š**

```bash
# .env.example

# AIè¨­å®š
VLLM_MODEL=openai/gpt-oss-20b
VLLM_QUANTIZATION=awq
VLLM_GPU_MEMORY_UTILIZATION=0.95
VLLM_MAX_MODEL_LEN=128000
VLLM_TENSOR_PARALLEL_SIZE=1

# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹
DATABASE_URL=postgresql://user:pass@localhost:5432/shodo
REDIS_URL=redis://localhost:6379

# ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£
JWT_SECRET_KEY=your-secret-key-here
TPM_DEVICE_PATH=/dev/tpm0
ENCRYPTION_KEY=your-encryption-key

# ã‚µãƒ¼ãƒ“ã‚¹è¨­å®š
API_PORT=8000
VLLM_PORT=8001
FRONTEND_PORT=3000

# ç›£æŸ»
AUDIT_LOG_PATH=/var/log/shodo/audit
AUDIT_RETENTION_DAYS=90

# ãƒ¬ãƒ¼ãƒˆåˆ¶é™
RATE_LIMIT_PER_MINUTE=60
RATE_LIMIT_PER_HOUR=1000
```

---

## **5. ã‚¤ãƒ³ãƒ•ãƒ©æ§‹æˆ**

### **5.1 ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢è¦ä»¶**

```yaml
é–‹ç™ºç’°å¢ƒ:
  GPU: RTX 4090 (24GB)
  CPU: AMD Ryzen 9 7950X (16ã‚³ã‚¢)
  RAM: 64GB DDR5
  SSD: 2TB NVMe Gen4
  æœˆé¡ã‚³ã‚¹ãƒˆ: 5ä¸‡å††
  
å°è¦æ¨¡æœ¬ç•ªï¼ˆã€œ1,000 req/dayï¼‰:
  GPU: RTX 4090 x1
  CPU: Intel Xeon E-2388G
  RAM: 128GB ECC
  SSD: 4TB NVMe RAID1
  æœˆé¡ã‚³ã‚¹ãƒˆ: 10ä¸‡å††
  
ä¸­è¦æ¨¡æœ¬ç•ªï¼ˆã€œ10,000 req/dayï¼‰:
  GPU: RTX 4090 x2 or A100 40GB x1
  CPU: AMD EPYC 7543
  RAM: 256GB ECC
  SSD: 8TB NVMe RAID10
  æœˆé¡ã‚³ã‚¹ãƒˆ: 30ä¸‡å††
  
å¤§è¦æ¨¡æœ¬ç•ªï¼ˆ10,000+ req/dayï¼‰:
  GPU: A100 80GB x2 or H100 x1
  CPU: Dual AMD EPYC 7763
  RAM: 512GB ECC
  SSD: 16TB NVMe RAID10
  æœˆé¡ã‚³ã‚¹ãƒˆ: 100ä¸‡å††
```

### **5.2 Docker Composeè¨­å®š**

```yaml
# docker-compose.yml
version: '3.9'

services:
  # GPT-OSSæ¨è«–ã‚µãƒ¼ãƒãƒ¼
  vllm:
    build: ./ai-server
    runtime: nvidia
    ports:
      - "8001:8001"
    environment:
      - CUDA_VISIBLE_DEVICES=0
      - MODEL_NAME=openai/gpt-oss-20b
      - QUANTIZATION=awq
    volumes:
      - ./models:/models
      - ./cache:/cache
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    
  # ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰API
  backend:
    build: ./backend
    ports:
      - "8000:8000"
    environment:
      - DATABASE_URL=${DATABASE_URL}
      - REDIS_URL=${REDIS_URL}
      - VLLM_URL=http://vllm:8001
    depends_on:
      - postgres
      - redis
      - vllm
    
  # ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰
  frontend:
    build: ./frontend
    ports:
      - "3000:3000"
    environment:
      - REACT_APP_API_URL=http://localhost:8000
    
  # User-Agent
  user-agent:
    build: ./user-agent
    environment:
      - PLAYWRIGHT_BROWSERS_PATH=/browsers
    volumes:
      - ./browser-data:/data
    
  # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹
  postgres:
    image: postgres:15
    environment:
      - POSTGRES_DB=shodo
      - POSTGRES_USER=shodo
      - POSTGRES_PASSWORD=secure_password
    volumes:
      - postgres_data:/var/lib/postgresql/data
    
  # ã‚­ãƒ£ãƒƒã‚·ãƒ¥
  redis:
    image: redis:7-alpine
    volumes:
      - redis_data:/data

volumes:
  postgres_data:
  redis_data:
```

---

## **6. é–‹ç™ºæ‰‹é †æ›¸**

### **6.1 é–‹ç™ºç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ï¼ˆ1æ—¥ç›®ï¼‰**

```bash
# 1. ãƒªãƒã‚¸ãƒˆãƒªã‚¯ãƒ­ãƒ¼ãƒ³
git clone https://github.com/your-org/shodo-ecosystem.git
cd shodo-ecosystem

# 2. GPT-OSSãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
python scripts/download_models.py --model gpt-oss-20b --quantization awq

# 3. ç’°å¢ƒå¤‰æ•°è¨­å®š
cp .env.example .env
# .envã‚’ç·¨é›†

# 4. Dockerç’°å¢ƒèµ·å‹•
docker-compose up -d

# 5. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–
docker-compose exec backend python -m alembic upgrade head
docker-compose exec backend python scripts/seed_data.py

# 6. å‹•ä½œç¢ºèª
curl http://localhost:8000/health
curl http://localhost:8001/v1/models
```

### **6.2 æ®µéšçš„å®Ÿè£…è¨ˆç”»**

#### **Week 1: åŸºç›¤æ§‹ç¯‰**

```bash
# Day 1-2: ç’°å¢ƒæ§‹ç¯‰ã¨GPT-OSSå°å…¥
- [ ] é–‹ç™ºç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
- [ ] GPT-OSSãƒ¢ãƒ‡ãƒ«è¨­å®š
- [ ] vLLMã‚µãƒ¼ãƒãƒ¼èµ·å‹•ç¢ºèª
- [ ] åŸºæœ¬çš„ãªæ¨è«–ãƒ†ã‚¹ãƒˆ

# Day 3-4: NLPåŸºç¤å®Ÿè£…
- [ ] äºŒé‡çµŒè·¯è§£æã‚¨ãƒ³ã‚¸ãƒ³
- [ ] ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹å‡¦ç†
- [ ] AIçµ±åˆ
- [ ] åŸºæœ¬çš„ãªæ„å›³è§£æ

# Day 5-7: APIåŸºç›¤
- [ ] FastAPIè¨­å®š
- [ ] GraphQL ã‚¹ã‚­ãƒ¼ãƒå®šç¾©
- [ ] åŸºæœ¬ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆå®Ÿè£…
- [ ] èªè¨¼åŸºç›¤
```

#### **Week 2: ã‚³ã‚¢æ©Ÿèƒ½å®Ÿè£…**

```bash
# Day 8-10: ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼æ©Ÿèƒ½
- [ ] ã‚µãƒ³ãƒ‰ãƒœãƒƒã‚¯ã‚¹ç’°å¢ƒæ§‹ç¯‰
- [ ] ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ç”Ÿæˆã‚¨ãƒ³ã‚¸ãƒ³
- [ ] å·®åˆ†è¨ˆç®—
- [ ] ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†

# Day 11-14: LPRã‚·ã‚¹ãƒ†ãƒ 
- [ ] èªè¨¼ãƒ•ãƒ­ãƒ¼å®Ÿè£…
- [ ] LPRç”Ÿæˆãƒ»æ¤œè¨¼
- [ ] ã‚¹ã‚³ãƒ¼ãƒ—ç®¡ç†
- [ ] ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å®Ÿè£…
```

#### **Week 3: çµ±åˆæ©Ÿèƒ½**

```bash
# Day 15-17: ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰
- [ ] SaaSæ¤œå‡ºã‚·ã‚¹ãƒ†ãƒ 
- [ ] ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ›´æ–°
- [ ] çµ±åˆUIå®Ÿè£…

# Day 18-21: MCPçµ±åˆ
- [ ] ãƒ„ãƒ¼ãƒ«ç™»éŒ²ã‚·ã‚¹ãƒ†ãƒ 
- [ ] å‹•çš„ãƒ•ã‚©ãƒ¼ãƒ ç”Ÿæˆ
- [ ] å®Ÿè¡Œã‚¨ãƒ³ã‚¸ãƒ³
```

#### **Week 4: å®Œæˆãƒ»æœ€é©åŒ–**

```bash
# Day 22-24: æœ€é©åŒ–
- [ ] ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
- [ ] ã‚­ãƒ£ãƒƒã‚·ãƒ¥æˆ¦ç•¥
- [ ] ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°

# Day 25-28: ãƒ†ã‚¹ãƒˆãƒ»ãƒ‡ãƒ—ãƒ­ã‚¤
- [ ] çµ±åˆãƒ†ã‚¹ãƒˆ
- [ ] è² è·ãƒ†ã‚¹ãƒˆ
- [ ] ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ç›£æŸ»
- [ ] æœ¬ç•ªãƒ‡ãƒ—ãƒ­ã‚¤
```

---

## **7. ãƒ†ã‚¹ãƒˆä»•æ§˜**

### **7.1 ãƒ†ã‚¹ãƒˆæˆ¦ç•¥**

```yaml
ãƒ†ã‚¹ãƒˆãƒ”ãƒ©ãƒŸãƒƒãƒ‰:
  å˜ä½“ãƒ†ã‚¹ãƒˆ: 70%
    - å„ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ
    - ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹æ¤œè¨¼
    - ãƒ¢ãƒƒã‚¯ä½¿ç”¨
    
  çµ±åˆãƒ†ã‚¹ãƒˆ: 20%
    - ã‚µãƒ¼ãƒ“ã‚¹é–“é€£æº
    - ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹çµ±åˆ
    - APIå‹•ä½œç¢ºèª
    
  E2Eãƒ†ã‚¹ãƒˆ: 10%
    - ä¸»è¦ã‚·ãƒŠãƒªã‚ª20æœ¬
    - UIæ“ä½œãƒ•ãƒ­ãƒ¼
    - å®Ÿç’°å¢ƒç›¸å½“
    
ã‚«ãƒãƒ¬ãƒƒã‚¸ç›®æ¨™:
  - ã‚³ãƒ¼ãƒ‰: 80%ä»¥ä¸Š
  - ãƒ–ãƒ©ãƒ³ãƒ: 70%ä»¥ä¸Š
  - ä¸»è¦ãƒ‘ã‚¹: 100%
```

### **7.2 ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ä¾‹**

```python
# tests/unit/test_nlp_engine.py
import pytest
from services.nlp import DualPathEngine

class TestNLPEngine:
    @pytest.fixture
    def engine(self):
        return DualPathEngine(test_config)
    
    async def test_japanese_analysis(self, engine):
        """æ—¥æœ¬èªè§£æãƒ†ã‚¹ãƒˆ"""
        result = await engine.analyze("Shopifyã®æ³¨æ–‡ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã—ã¦")
        
        assert result.intent == "export"
        assert result.service == "shopify"
        assert result.target == "orders"
        assert result.confidence > 0.8
    
    async def test_ambiguity_resolution(self, engine):
        """æ›–æ˜§æ€§è§£æ±ºãƒ†ã‚¹ãƒˆ"""
        result = await engine.analyze("ã‚‚ã£ã¨å¤§ãã")
        
        assert result.requires_confirmation == True
        assert len(result.suggestions) >= 3
        assert result.confidence < 0.7
    
    async def test_refinement(self, engine):
        """ä¿®æ­£å‡¦ç†ãƒ†ã‚¹ãƒˆ"""
        initial = await engine.analyze("ä¾¡æ ¼ã‚’å¤‰æ›´")
        refined = await engine.refine(initial, "1000å††ã«")
        
        assert refined.entities['price'] == 1000
        assert refined.entities['currency'] == 'JPY'
```

---

## **8. é‹ç”¨ä»•æ§˜**

### **8.1 ç›£è¦–é …ç›®**

```yaml
ã‚¤ãƒ³ãƒ•ãƒ©ãƒ¡ãƒˆãƒªã‚¯ã‚¹:
  - GPUä½¿ç”¨ç‡: < 80%
  - GPUãƒ¡ãƒ¢ãƒª: < 22GB
  - CPUä½¿ç”¨ç‡: < 70%
  - ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡: < 80%
  - ãƒ‡ã‚£ã‚¹ã‚¯I/O: < 1000 IOPS

ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¡ãƒˆãƒªã‚¯ã‚¹:
  - ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚¿ã‚¤ãƒ : p95 < 500ms
  - ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: > 100 req/min
  - ã‚¨ãƒ©ãƒ¼ãƒ¬ãƒ¼ãƒˆ: < 1%
  - LPRç”ŸæˆæˆåŠŸç‡: > 99%

ãƒ“ã‚¸ãƒã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹:
  - æ—¥æ¬¡ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ¦ãƒ¼ã‚¶ãƒ¼
  - å‡¦ç†ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°
  - ã‚µãƒ¼ãƒ“ã‚¹åˆ¥åˆ©ç”¨ç‡
  - ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿé »åº¦
```

### **8.2 é‹ç”¨æ‰‹é †**

```bash
# æ—¥æ¬¡é‹ç”¨
- ãƒ­ã‚°ç¢ºèª
- ãƒ¡ãƒˆãƒªã‚¯ã‚¹ç¢ºèª
- ã‚¢ãƒ©ãƒ¼ãƒˆå¯¾å¿œ

# é€±æ¬¡é‹ç”¨  
- ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒ“ãƒ¥ãƒ¼
- ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ‘ãƒƒãƒé©ç”¨
- ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ç¢ºèª

# æœˆæ¬¡é‹ç”¨
- ã‚­ãƒ£ãƒ‘ã‚·ãƒ†ã‚£ãƒ—ãƒ©ãƒ³ãƒ‹ãƒ³ã‚°
- ã‚³ã‚¹ãƒˆæœ€é©åŒ–
- ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ç›£æŸ»
```

---

## **ã¾ã¨ã‚**

æœ¬ä»•æ§˜æ›¸ã¯ã€**GPT-OSS-20B**ã‚’æ´»ç”¨ã—ãŸ**shodo ecosystem**ã®å®Œå…¨ãªå®Ÿè£…ã‚¬ã‚¤ãƒ‰ã§ã™ã€‚

### **å®Ÿç¾ã•ã‚Œã‚‹ä¾¡å€¤**

1. **ã‚³ã‚¹ãƒˆå‰Šæ¸›**: æœˆé¡200ä¸‡å†† â†’ 5ä¸‡å††ï¼ˆ97.5%å‰Šæ¸›ï¼‰
2. **æ€§èƒ½å‘ä¸Š**: 15å€ã®é«˜é€ŸåŒ–
3. **ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£**: å®Œå…¨ãƒ­ãƒ¼ã‚«ãƒ«å‡¦ç†
4. **æ‹¡å¼µæ€§**: UIæ”¹ä¿®ä¸è¦ã§æ–°æ©Ÿèƒ½è¿½åŠ 

### **æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—**

1. é–‹ç™ºç’°å¢ƒæ§‹ç¯‰ï¼ˆ1æ—¥ï¼‰
2. MVPå®Ÿè£…ï¼ˆ1é€±é–“ï¼‰
3. æœ¬ç•ªå±•é–‹ï¼ˆ1ãƒ¶æœˆï¼‰

**æœˆé¡5ä¸‡å††ã§ã€ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚ºã‚°ãƒ¬ãƒ¼ãƒ‰ã®AIçµ±åˆã‚·ã‚¹ãƒ†ãƒ ãŒå®Ÿç¾å¯èƒ½ã§ã™ã€‚**
